{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3369458,"sourceType":"datasetVersion","datasetId":2031850},{"sourceId":10778669,"sourceType":"datasetVersion","datasetId":6687878}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-21T13:09:49.772334Z","iopub.execute_input":"2025-02-21T13:09:49.772654Z","iopub.status.idle":"2025-02-21T13:09:50.797865Z","shell.execute_reply.started":"2025-02-21T13:09:49.772624Z","shell.execute_reply":"2025-02-21T13:09:50.797142Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import Libraries\nimport os\nimport gc\nimport shutil\nimport cv2\nimport numpy as np \nimport pandas as pd\nfrom tqdm import tqdm\ntqdm.pandas()\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n #Data Visualization\nfrom glob import glob\nimport xml.etree.ElementTree as xet\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport plotly\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nfrom IPython.display import Image, display","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T13:09:50.798939Z","iopub.execute_input":"2025-02-21T13:09:50.799338Z","iopub.status.idle":"2025-02-21T13:09:51.942322Z","shell.execute_reply.started":"2025-02-21T13:09:50.799306Z","shell.execute_reply":"2025-02-21T13:09:51.941444Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import xml.etree.ElementTree as ET  # or use your alias, e.g., import xml.etree.ElementTree as xet\nfrom glob import glob\n\ndef append_labels_from_xml(glob_path, labels_dict):\n    \"\"\"\n    Processes XML files from a given glob path and appends data to the labels_dict.\n    \n    Parameters:\n        glob_path (str): The glob pattern to find XML files.\n        labels_dict (dict): A dictionary with keys 'filepath', 'xmin', 'xmax', 'ymin', 'ymax', 'plate_number'.\n                            Data from each XML file will be appended to these lists.\n    \"\"\"\n    # Get a list of files matching the glob pattern\n    file_list = glob(glob_path)\n    #print(f\"Processing {len(file_list)} files from: {glob_path}\")\n    \n    for filename in file_list:\n        tree = ET.parse(filename)\n        root = tree.getroot()\n        #print(\"Path is:\", root.find('path').text)\n        \n        # Extract the necessary information\n        obj = root.find('object')\n        plate_number = obj.find('name').text.strip()\n        bndbox = obj.find('bndbox')\n        xmin = int(bndbox.find('xmin').text)\n        xmax = int(bndbox.find('xmax').text)\n        ymin = int(bndbox.find('ymin').text)\n        ymax = int(bndbox.find('ymax').text)\n        \n        # Append the extracted data to the provided dictionary\n        labels_dict['filepath'].append(filename)\n        labels_dict['xmin'].append(xmin)\n        labels_dict['xmax'].append(xmax)\n        labels_dict['ymin'].append(ymin)\n        labels_dict['ymax'].append(ymax)\n        labels_dict['plate_number'].append(plate_number)\n\n# Example usage:\n\n# Initialize your dictionary (this can later be converted to a DataFrame)\nnew_labels_dict = dict(filepath=[], xmin=[], xmax=[], ymin=[], ymax=[], plate_number=[])\n\nnew_test_labels_dict = dict(filepath=[], xmin=[], xmax=[], ymin=[], ymax=[], plate_number=[])\n\nnew_labels_video_dict = dict(filepath=[], xmin=[], xmax=[], ymin=[], ymax=[], plate_number=[])\n \n# Process files from one glob path\nappend_labels_from_xml('/kaggle/input/indian-vehicle-dataset/State-wise_OLX/*/*.xml', new_labels_dict)\nappend_labels_from_xml('/kaggle/input/indian-vehicle-dataset/video_images/*.xml', new_labels_video_dict)\nappend_labels_from_xml('/kaggle/input/indian-vehicle-dataset/google_images/*.xml', new_test_labels_dict)\n\n# Now, new_labels_dict contains appended data from all the XML files processed.\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T13:09:51.944188Z","iopub.execute_input":"2025-02-21T13:09:51.944665Z","iopub.status.idle":"2025-02-21T13:10:05.591555Z","shell.execute_reply.started":"2025-02-21T13:09:51.944641Z","shell.execute_reply":"2025-02-21T13:10:05.590781Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_olx = pd.DataFrame(new_labels_dict)\ndf_olx.to_csv('labels_train.csv',index=False)\nprint(df_olx.head())\ndf_olx.count()\n\ndf_google = pd.DataFrame(new_test_labels_dict)\ndf_google.to_csv('labels_test.csv',index=False)\nprint(df_google.head())\ndf_google.count()\n\ndf_video = pd.DataFrame(new_labels_video_dict)\ndf_video.to_csv('labels_train.csv',index=False)\nprint(df_video.head())\ndf_video.count()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T13:10:05.592943Z","iopub.execute_input":"2025-02-21T13:10:05.593263Z","iopub.status.idle":"2025-02-21T13:10:05.639092Z","shell.execute_reply.started":"2025-02-21T13:10:05.593216Z","shell.execute_reply":"2025-02-21T13:10:05.638475Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# parsing\ndef addcolumns_train(path):\n    parser = xet.parse(path).getroot()\n    name = parser.find('filename').text\n    filename = f'/kaggle/input/indian-vehicle-dataset/State-wise_OLX/{name[:2]}/{name}'\n\n    # width and height\n    parser_size = parser.find('size')\n    width = int(parser_size.find('width').text)\n    height = int(parser_size.find('height').text)\n    \n    return filename, width, height\n\ndf_olx[['filename','width','height']] = df_olx['filepath'].apply(addcolumns_train).apply(pd.Series)\ndf_olx.head()\ndf_olx.count()\n\n\n# parsing\ndef addcolumns_test(path):\n    parser = xet.parse(path).getroot()\n    name = parser.find('filename').text\n    filename = f'/kaggle/input/indian-vehicle-dataset/google_images/{name}'\n    # width and height\n    parser_size = parser.find('size')\n    width = int(parser_size.find('width').text)\n    height = int(parser_size.find('height').text)\n    \n    return filename, width, height\n\ndf_google[['filename','width','height']] = df_google['filepath'].apply(addcolumns_test).apply(pd.Series)\ndf_google.head()\ndf_google.count()\n\n\n# parsing\ndef addcolumns_video_train(path):\n    parser = xet.parse(path).getroot()\n    name = parser.find('filename').text\n    filename = f'/kaggle/input/indian-vehicle-dataset/video_images/{name}'\n\n    # width and height\n    parser_size = parser.find('size')\n    width = int(parser_size.find('width').text)\n    height = int(parser_size.find('height').text)\n    \n    return filename, width, height\n\ndf_video[['filename','width','height']] = df_video['filepath'].apply(addcolumns_video_train).apply(pd.Series)\ndf_video.head()\ndf_video.count()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T13:10:05.640046Z","iopub.execute_input":"2025-02-21T13:10:05.640372Z","iopub.status.idle":"2025-02-21T13:10:06.577967Z","shell.execute_reply.started":"2025-02-21T13:10:05.640337Z","shell.execute_reply":"2025-02-21T13:10:06.577072Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # center_x, center_y, width , height\n# df['center_x'] = (df['xmax'] + df['xmin'])/(2*df['width'])\n# df['center_y'] = (df['ymax'] + df['ymin'])/(2*df['height'])\n\n# df['bb_width'] = (df['xmax'] - df['xmin'])/df['width']\n# df['bb_height'] = (df['ymax'] - df['ymin'])/df['height']\n# df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T13:10:06.578870Z","iopub.execute_input":"2025-02-21T13:10:06.579146Z","iopub.status.idle":"2025-02-21T13:10:06.582500Z","shell.execute_reply.started":"2025-02-21T13:10:06.579115Z","shell.execute_reply":"2025-02-21T13:10:06.581535Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df = df.rename(columns={\"filename\": \"image_path\"})\n# # Extract individual bbox coordinates\n# df[\"x1_bbox\"] = df[\"xmin\"] \n# df[\"y1_bbox\"] = df[\"ymin\"] \n# df[\"x2_bbox\"] = df[\"xmax\"] \n# df[\"y2_bbox\"] = df[\"ymax\"]\n# df[\"xmid\"] = df[\"center_x\"]\n# df[\"ymid\"] = df[\"center_y\"]\n# df = df.drop(columns=[\"filepath\", \"xmin\", \"xmax\", \"ymin\", \"ymax\", \"width\", \"height\", \"center_x\", \"center_y\"])\n# df.count()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T13:10:06.583199Z","iopub.execute_input":"2025-02-21T13:10:06.583433Z","iopub.status.idle":"2025-02-21T13:10:06.599461Z","shell.execute_reply.started":"2025-02-21T13:10:06.583411Z","shell.execute_reply":"2025-02-21T13:10:06.598682Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_dataframe(df):\n    \"\"\"\n    Modifies the given DataFrame in place by performing the following operations:\n    \n    1. Computes 'center_x' and 'center_y' columns as:\n         center_x = (xmax + xmin) / (2 * width)\n         center_y = (ymax + ymin) / (2 * height)\n    \n    2. Computes bounding box dimensions 'bb_width' and 'bb_height' as:\n         bb_width  = (xmax - xmin) / width\n         bb_height = (ymax - ymin) / height\n    \n    3. Renames the column 'filename' to 'image_path'.\n    \n    4. Creates new columns for individual bounding box coordinates:\n         - x1_bbox from xmin\n         - y1_bbox from ymin\n         - x2_bbox from xmax\n         - y2_bbox from ymax\n         - xmid from center_x\n         - ymid from center_y\n    \n    5. Drops the unnecessary columns:\n         ['filepath', 'xmin', 'xmax', 'ymin', 'ymax', 'width', 'height', 'center_x', 'center_y']\n    \"\"\"\n    # Compute center_x and center_y\n    df['center_x'] = (df['xmax'] + df['xmin']) / (2 * df['width'])\n    df['center_y'] = (df['ymax'] + df['ymin']) / (2 * df['height'])\n    \n    # Compute bounding box width and height\n    df['bb_width'] = (df['xmax'] - df['xmin']) / df['width']\n    df['bb_height'] = (df['ymax'] - df['ymin']) / df['height']\n    \n    # Rename the column 'filename' to 'image_path' in place\n    df.rename(columns={\"filename\": \"image_path\"}, inplace=True)\n    \n    # Extract individual bbox coordinates and midpoints\n    df[\"x1_bbox\"] = df[\"xmin\"]\n    df[\"y1_bbox\"] = df[\"ymin\"]\n    df[\"x2_bbox\"] = df[\"xmax\"]\n    df[\"y2_bbox\"] = df[\"ymax\"]\n    df[\"xmid\"] = df[\"center_x\"]\n    df[\"ymid\"] = df[\"center_y\"]\n    \n    # Drop the unnecessary columns\n    columns_to_drop = [\"filepath\", \"xmin\", \"xmax\", \"ymin\", \"ymax\", \"width\", \"height\", \"center_x\", \"center_y\"]\n    df.drop(columns=columns_to_drop, inplace=True)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T13:10:06.601794Z","iopub.execute_input":"2025-02-21T13:10:06.601980Z","iopub.status.idle":"2025-02-21T13:10:06.613606Z","shell.execute_reply.started":"2025-02-21T13:10:06.601964Z","shell.execute_reply":"2025-02-21T13:10:06.612856Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Processing olx train data\")\nprocess_dataframe(df_olx)\nprint(df_olx.head())\nprint(\"Processing google test data\")\nprocess_dataframe(df_google)\nprint(df_google.head())\n\nprint(\"Processing video train data\")\nprocess_dataframe(df_video)\nprint(df_video.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T13:10:06.614795Z","iopub.execute_input":"2025-02-21T13:10:06.615056Z","iopub.status.idle":"2025-02-21T13:10:06.664202Z","shell.execute_reply.started":"2025-02-21T13:10:06.615037Z","shell.execute_reply":"2025-02-21T13:10:06.663439Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train_olx_video = pd.concat([df_olx,df_video]).sample(frac=1).reset_index(drop=True)\ndf_train_olx_video.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T13:10:06.665038Z","iopub.execute_input":"2025-02-21T13:10:06.665366Z","iopub.status.idle":"2025-02-21T13:10:06.687453Z","shell.execute_reply.started":"2025-02-21T13:10:06.665343Z","shell.execute_reply":"2025-02-21T13:10:06.686747Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train_olx_video.count()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T13:10:06.688100Z","iopub.execute_input":"2025-02-21T13:10:06.688332Z","iopub.status.idle":"2025-02-21T13:10:06.694554Z","shell.execute_reply.started":"2025-02-21T13:10:06.688309Z","shell.execute_reply":"2025-02-21T13:10:06.693877Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nfrom sklearn.model_selection import train_test_split\n\n\n\n# Split the data into train and test sets\ntrain_df, test_df = train_test_split(df_train_olx_video, test_size=0.2, random_state=42)\n\n# Create directories for YOLO dataset\nos.makedirs(\"/kaggle/working/yolo_dataset/train/images\", exist_ok=True)\nos.makedirs(\"/kaggle/working/yolo_dataset/train/labels\", exist_ok=True)\nos.makedirs(\"/kaggle/working/yolo_dataset/test/images\", exist_ok=True)\nos.makedirs(\"/kaggle/working/yolo_dataset/test/labels\", exist_ok=True)\n\n# Convert dataset format to YOLO\ndef convert_to_yolo(row, img_width, img_height):\n    x_center = ((row['x1_bbox'] + row['x2_bbox']) / 2) / img_width\n    y_center = ((row['y1_bbox'] + row['y2_bbox']) / 2) / img_height\n    width = row['bb_width']\n    height = row['bb_height']\n    return f\"0 {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\"\n\n\ndef create_yolo_dataset(is_train,df):\n\n    if is_train:\n        folder_type = 'train'\n    else:\n        folder_type = 'test'\n\n    print(\"folder type \",folder_type)\n    skipped_images = 0\n    \n    # Process images and labels\n    \n#   for index, row in tqdm(df.iterrows(), total=len(df)):\n    for index, row in df.iterrows():\n        img_path = row['image_path']\n        #print(f\"Trying to read: {img_path}\")  # Debugging step\n        img = cv2.imread(img_path)\n\n        if img is None:\n            print(f\"⚠ Warning: Could not load image {img_path}..skipping\")\n            skipped_images += 1\n            continue\n    \n        h, w, _ = img.shape  # Get image size\n    \n        # Convert bbox to YOLO format\n        yolo_annotation = convert_to_yolo(row, img_width=w, img_height=h)\n    \n        # Save image\n        img_filename = f\"/kaggle/working/yolo_dataset/{folder_type}/images/{index}.jpg\"\n        cv2.imwrite(img_filename, img)\n    \n        # Save label\n        label_filename = f\"/kaggle/working/yolo_dataset/{folder_type}/labels/{index}.txt\"\n        with open(label_filename, \"w\") as f:\n            f.write(yolo_annotation)\n    print(\"skipped images\", skipped_images)\n\ncreate_yolo_dataset(True,train_df)\ncreate_yolo_dataset(False,test_df)\n\nprint(\"✅ Data conversion complete. Ready for YOLO training.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T13:10:06.695325Z","iopub.execute_input":"2025-02-21T13:10:06.695588Z","iopub.status.idle":"2025-02-21T13:10:28.533320Z","shell.execute_reply.started":"2025-02-21T13:10:06.695565Z","shell.execute_reply":"2025-02-21T13:10:28.532508Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.count()\ntest_df.count()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T13:10:28.534059Z","iopub.execute_input":"2025-02-21T13:10:28.534574Z","iopub.status.idle":"2025-02-21T13:10:28.542678Z","shell.execute_reply.started":"2025-02-21T13:10:28.534550Z","shell.execute_reply":"2025-02-21T13:10:28.541824Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a data.yaml file with the correct paths\n\nimport yaml\n\ndata_dict = {\n    'train': '/kaggle/working/yolo_dataset/train/images',\n    'val': '/kaggle/working/yolo_dataset/test/images',\n    'nc': 1,  # Update based on the number of classes in your dataset\n    'names': ['license_plate']  # Update based on your class names\n}\n\nwith open('data.yaml', 'w') as f:\n    yaml.dump(data_dict, f)\n\nprint(\"data.yaml file created successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T13:10:28.543474Z","iopub.execute_input":"2025-02-21T13:10:28.543747Z","iopub.status.idle":"2025-02-21T13:10:28.580926Z","shell.execute_reply.started":"2025-02-21T13:10:28.543698Z","shell.execute_reply":"2025-02-21T13:10:28.580291Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install ultralytics\n\n\nimport ultralytics\nfrom ultralytics import YOLO\n\n# Train YOLO on the dataset\nmodel = YOLO(\"yolov8n.pt\")  # Use a pre-trained model\n\n\n\n# Train YOLO with custom dataset\nmodel.train(data=\"data.yaml\", epochs=50, imgsz=640)\n\n# Save the model to a specific path\nmodel.save('/kaggle/working/mymodel.pt')\n\n# Load YOLO model\n\n#model = YOLO(\"/kaggle/working/runs/detect/train/weights/best.pt\")  # Path to trained model\nmodel = YOLO('/kaggle/working/mymodel.pt')\nprint(\"Not using Yolov8N\")\nprint(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T13:10:28.581603Z","iopub.execute_input":"2025-02-21T13:10:28.581844Z","iopub.status.idle":"2025-02-21T13:21:57.187469Z","shell.execute_reply.started":"2025-02-21T13:10:28.581814Z","shell.execute_reply":"2025-02-21T13:21:57.186403Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import re\n\n# def extract_license_plate(ocr_result, confidence_threshold=0.5, size_threshold_factor=0.5):\n#     # If OCR result is empty or improperly formatted, return a default value\n#     if not ocr_result or not ocr_result[0]:\n#         return \"No license plate detected\"\n    \n#     # Initialize a list to hold texts that pass filtering\n#     detected_texts = []\n#     max_area = 0  # To track the largest detected text area\n\n#     # Loop through OCR detections\n#     for detection in ocr_result[0]:  # Extract detections\n#         if len(detection) > 0:\n#             text = detection[1][0]  # Extract the text\n#             confidence = detection[1][1]  # Extract confidence score\n#             box = detection[0]  # Bounding box (coordinates)\n            \n#             # Calculate the area of the bounding box (width * height)\n#             width = abs(box[1][0] - box[0][0])\n#             height = abs(box[2][1] - box[1][1])\n#             area = width * height\n\n#             # Track maximum area for size comparison\n#             max_area = max(max_area, area)\n\n#             # Filter based on confidence score and size (compared to largest detected text area)\n#             if confidence > confidence_threshold and area > (max_area * size_threshold_factor):\n#                 detected_texts.append(text)\n\n#     # Concatenate all filtered texts into one string\n#     final_text = \" \".join(detected_texts)\n    \n#     # Remove non-alphanumeric characters using regex\n#     cleaned_text = re.sub(r'[^A-Za-z0-9]', '', final_text)\n    \n#     return cleaned_text\n\n# # Example OCR result\n# ocr_result = [[[[[12.0, 21.0], [161.0, 22.0], [160.0, 39.0], [12.0, 37.0]], ('MH 20BQ20', 0.8835029006004333)]], \n#                [[[[12.0, 41.0], [70.0, 40.0], [69.0, 60.0], [12.0, 61.0]], ('SMALL', 0.45)]]]\n\n# # Extract license plate number with highest confidence\n# license_plate_text = extract_license_plate(ocr_result)\n# print(\"Extracted License Plate:\", license_plate_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T13:21:57.188810Z","iopub.execute_input":"2025-02-21T13:21:57.189174Z","iopub.status.idle":"2025-02-21T13:21:57.193801Z","shell.execute_reply.started":"2025-02-21T13:21:57.189130Z","shell.execute_reply":"2025-02-21T13:21:57.192814Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nimport xml.etree.ElementTree as ET\nfrom glob import glob\nimport numpy as np\n\n# Define a simple confusion mapping for common OCR errors\ndigit_to_letter = {'0': 'O', '1': 'I', '2': 'Z', '5': 'S', '6': 'G', '8': 'B', '9': 'P'}\nletter_to_digit = {v: k for k, v in digit_to_letter.items()}\n\ndef extract_license_plate(ocr_result, confidence_threshold=0.5, size_threshold_factor=0.5):\n    \"\"\"\n    Extracts and cleans text from OCR output, then applies a simple HMM correction\n    based on the expected license plate format: AAXXAAXXXX (where A=alphabet, X=digit).\n    \"\"\"\n    # If OCR result is empty or improperly formatted, return a default value\n    if not ocr_result or not ocr_result[0]:\n        return \"No license plate detected\"\n    \n    # Initialize a list to hold texts that pass filtering\n    detected_texts = []\n    max_area = 0  # To track the largest detected text area\n\n    # Loop through OCR detections\n    for detection in ocr_result[0]:  # Assuming detections are in the first element\n        if len(detection) > 0:\n            text = detection[1][0]          # Extract the text\n            confidence = detection[1][1]      # Extract confidence score\n            box = detection[0]                # Bounding box (coordinates)\n            \n            # Calculate the area of the bounding box (width * height)\n            width = abs(box[1][0] - box[0][0])\n            height = abs(box[2][1] - box[1][1])\n            area = width * height\n\n            # Update maximum area for size comparison\n            max_area = max(max_area, area)\n\n            # Filter based on confidence score and size (compared to largest detected text area)\n            if confidence > confidence_threshold and area > (max_area * size_threshold_factor):\n                detected_texts.append(text)\n\n    # Concatenate all filtered texts into one string\n    final_text = \" \".join(detected_texts)\n    \n    # Remove non-alphanumeric characters using regex\n    cleaned_text = re.sub(r'[^A-Za-z0-9]', '', final_text)\n    \n    # --- HMM Correction Step ---\n    # We expect a license plate of format: AAXXAAXXXX \n    # (positions 0-1 and 4-5 are letters; positions 2-3 and 6-9 are digits)\n    \n    def hmm_correct_plate(obs_text):\n        expected_types = ['letter', 'letter', 'digit', 'digit', \n                          'letter', 'letter', 'digit', 'digit', 'digit', 'digit']\n        n = len(expected_types)\n        \n        # If the observed text is not 10 characters, you might want to handle it differently.\n        # For now, we only correct if we have exactly 10 characters.\n        if len(obs_text) != n:\n            return obs_text  # Or return an error/default value\n        \n        # Candidate sets for each expected type\n        letter_candidates = list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\")\n        digit_candidates = list(\"0123456789\")\n        \n        # Define emission probability function.\n        # We assume:\n        #   - if observed char is of the expected type and matches candidate: 0.9\n        #   - if observed char is of the expected type but not equal: small probability\n        #   - if observed char is of the opposite type but can be mapped via confusion: moderate probability\n        #   - otherwise, very low probability.\n        def emission_prob(expected_type, candidate, observed):\n            candidate = candidate.upper() if expected_type == 'letter' else candidate\n            observed = observed.upper() if expected_type == 'letter' else observed\n            if expected_type == 'letter':\n                if observed.isalpha():\n                    return 0.9 if candidate == observed else 0.1 / (len(letter_candidates)-1)\n                elif observed.isdigit():\n                    # Allow mapping from digit to letter if common OCR confusion exists\n                    if digit_to_letter.get(observed, None) == candidate:\n                        return 0.5\n                    else:\n                        return 0.01\n                else:\n                    return 0.01\n            else:  # expected digit\n                if observed.isdigit():\n                    return 0.9 if candidate == observed else 0.1 / (len(digit_candidates)-1)\n                elif observed.isalpha():\n                    if letter_to_digit.get(observed, None) == candidate:\n                        return 0.5\n                    else:\n                        return 0.01\n                else:\n                    return 0.01\n        \n        # For simplicity, we use uniform (or identity) transition probabilities.\n        # Our state sequence is fixed by the format so each position is independent given the observation.\n        # We still demonstrate a Viterbi algorithm.\n        \n        dp = []         # dp[i] will be a dict mapping candidate char at position i to probability\n        backpointer = []  # To recover the best sequence\n        \n        # Initialization for position 0\n        current_candidates = letter_candidates if expected_types[0] == 'letter' else digit_candidates\n        dp0 = {}\n        bp0 = {}\n        for c in current_candidates:\n            dp0[c] = emission_prob(expected_types[0], c, obs_text[0])\n            bp0[c] = None\n        dp.append(dp0)\n        backpointer.append(bp0)\n        \n        # Recursion for positions 1 to n-1\n        for i in range(1, n):\n            current_candidates = letter_candidates if expected_types[i] == 'letter' else digit_candidates\n            dp_curr = {}\n            bp_curr = {}\n            for curr in current_candidates:\n                max_prob = -1\n                best_prev = None\n                # Since transition probabilities are uniform, we only multiply by emission probability.\n                for prev, prev_prob in dp[i-1].items():\n                    prob = prev_prob * emission_prob(expected_types[i], curr, obs_text[i])\n                    if prob > max_prob:\n                        max_prob = prob\n                        best_prev = prev\n                dp_curr[curr] = max_prob\n                bp_curr[curr] = best_prev\n            dp.append(dp_curr)\n            backpointer.append(bp_curr)\n        \n        # Termination: pick the candidate at the last position with maximum probability.\n        last_candidates = dp[-1]\n        best_last = max(last_candidates, key=last_candidates.get)\n        \n        # Backtrace to retrieve the best candidate sequence.\n        best_sequence = [best_last]\n        for i in range(n-1, 0, -1):\n            best_sequence.insert(0, backpointer[i][best_sequence[0]])\n        \n        return \"\".join(best_sequence)\n    \n    # Apply HMM correction if the cleaned text is 10 characters long;\n    # otherwise, return the cleaned text as is.\n    \n    if len(cleaned_text) == 10:\n        print(\"calling hmm corrct plate for\",cleaned_text)\n        corrected_text = hmm_correct_plate(cleaned_text)\n        print(\"corrected text\",corrected_text)\n\n    else:\n        print(\"directly assigned cleaned text\")\n        corrected_text = cleaned_text\n\n    corrected_text = cleaned_text\n    return corrected_text\n\n# --- Example OCR result ---\nocr_result = [\n    [  # First group of detections (we assume these are the license plate candidates)\n        [\n            [[12.0, 21.0], [161.0, 22.0], [160.0, 39.0], [12.0, 37.0]],\n            ('A82BDD1963', 0.8835029006004333)\n        ]\n    ],\n    [  # A second detection that might be filtered out (low confidence, etc.)\n        [\n            [[12.0, 41.0], [70.0, 40.0], [69.0, 60.0], [12.0, 61.0]],\n            ('SMALL', 0.45)\n        ]\n    ]\n]\n\n# Extract and correct the license plate number\nlicense_plate_text = extract_license_plate(ocr_result)\nprint(\"Extracted License Plate:\", license_plate_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T13:21:57.194932Z","iopub.execute_input":"2025-02-21T13:21:57.195288Z","iopub.status.idle":"2025-02-21T13:21:57.217213Z","shell.execute_reply.started":"2025-02-21T13:21:57.195255Z","shell.execute_reply":"2025-02-21T13:21:57.216300Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\n\n# Mapping common OCR misreads\ndigit_to_letter = {'0': 'O', '1': 'I', '2': 'Z', '5': 'S', '6': 'G', '8': 'B', '9': 'P'}\nletter_to_digit = {v: k for k, v in digit_to_letter.items()}\n\ndef extract_license_plate_easyocr(ocr_result, confidence_threshold=0.5):\n    \"\"\"\n    Extracts and cleans text from EasyOCR output.\n\n    Args:\n        ocr_result (list): Output from EasyOCR, formatted as [[bbox, text, confidence], ...].\n        confidence_threshold (float): Minimum confidence to consider a detection.\n\n    Returns:\n        str: Extracted and corrected license plate text.\n    \"\"\"\n    if not ocr_result:\n        return \"No license plate detected\"\n    \n    detected_texts = []\n    \n    for detection in ocr_result:\n        bbox, text, confidence = detection  # Unpack the detection\n        \n        if confidence > confidence_threshold:\n            detected_texts.append(text)\n\n    # Combine detected text parts\n    final_text = \" \".join(detected_texts)\n\n    # Remove non-alphanumeric characters\n    cleaned_text = re.sub(r'[^A-Za-z0-9]', '', final_text)\n\n    return cleaned_text\n\n# --- Example EasyOCR result ---\neasyocr_result = [\n    [[12, 21, 161, 22], 'A82BDD1963', 0.88],\n    [[12, 41, 70, 40], 'SMALL', 0.45]  # This will be ignored due to low confidence\n]\n\n# Extract the license plate\nlicense_plate_text = extract_license_plate_easyocr(easyocr_result)\nprint(\"Extracted License Plate:\", license_plate_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T13:21:57.218158Z","iopub.execute_input":"2025-02-21T13:21:57.218478Z","iopub.status.idle":"2025-02-21T13:21:57.237299Z","shell.execute_reply.started":"2025-02-21T13:21:57.218447Z","shell.execute_reply":"2025-02-21T13:21:57.236525Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install paddleocr\n!pip install paddlepaddle\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T13:21:57.238153Z","iopub.execute_input":"2025-02-21T13:21:57.238484Z","iopub.status.idle":"2025-02-21T13:22:20.032760Z","shell.execute_reply.started":"2025-02-21T13:21:57.238455Z","shell.execute_reply":"2025-02-21T13:22:20.031681Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install python-Levenshtein\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T13:22:20.034074Z","iopub.execute_input":"2025-02-21T13:22:20.034470Z","iopub.status.idle":"2025-02-21T13:22:24.193084Z","shell.execute_reply.started":"2025-02-21T13:22:20.034429Z","shell.execute_reply":"2025-02-21T13:22:24.192085Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model = YOLO(\"/kaggle/input/apr/pytorch/default/1/ANPR_Best21.pt\")  # Path to trained model\n# print('model loaded')\n# print(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T13:22:24.194187Z","iopub.execute_input":"2025-02-21T13:22:24.194529Z","iopub.status.idle":"2025-02-21T13:22:24.198563Z","shell.execute_reply.started":"2025-02-21T13:22:24.194501Z","shell.execute_reply":"2025-02-21T13:22:24.197599Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom ultralytics import YOLO\nimport cv2\nimport os\nimport easyocr\nfrom difflib import SequenceMatcher\nfrom tqdm import tqdm\nfrom paddleocr import PaddleOCR\nimport logging\nimport Levenshtein\n\nclasses = [\"Licence Plate\"]\n\n# List to store OCR results\ndetected_plates = []\n\n# Accuracy counters\ncorrect_predictions = 0\ntotal_characters = 0\ncharacter_accuracy_sum = 0\nlevenshtein_accuracy_sum = 0\n\n# Function to calculate character-level accuracy\ndef character_level_accuracy(ocr_text, ground_truth):\n    # Remove any extra spaces\n   # Clean the OCR text by joining parts and removing spaces\n    ocr_text = ''.join(ocr_text).replace(\" \", \"\").lower()\n    print(\"final ocr_text\",ocr_text)\n    ground_truth = ground_truth.replace(\" \", \"\").lower()\n    \n    correct_characters = 0\n    invalid_images = 0\n    total_characters = len(ground_truth)\n    \n    # Compare each character in OCR and ground truth\n    for i in range(min(len(ocr_text), len(ground_truth))):\n        if ocr_text[i] == ground_truth[i]:\n            correct_characters += 1\n    \n    return correct_characters / total_characters if total_characters > 0 else 0\n\n# Function to calculate word-level accuracy (Exact match)\ndef word_level_accuracy(ocr_text, ground_truth):\n    ocr_text = ocr_text.upper()\n    ground_truth = ground_truth.upper()\n    return 1 if ocr_text.strip() == ground_truth.strip() else 0\n\n# Function to calculate Levenshtein Distance-based accuracy\ndef levenshtein_distance_accuracy(ocr_text, ground_truth):\n    \"\"\"Computes normalized Levenshtein-based accuracy\"\"\"\n    distance = Levenshtein.distance(ocr_text, ground_truth)\n    max_len = max(len(ocr_text), len(ground_truth))\n    return (1 - distance / max_len) if max_len > 0 else 0\n    \nprint(\"Reset variables\")\ncorrect_characters = 0\ninvalid_images = 0\n\n# Process each image\n#for index, row in tqdm(df_google.iterrows(), total=len(df_google)):\nfor index, row in tqdm(df_google.iterrows(), total=10):\n\n    image_path = row[\"image_path\"]\n    ground_truth = row[\"plate_number\"]\n    \n    image = cv2.imread(image_path)\n    if image is None:\n        print(f\"⚠️ Error: Unable to load image at {image_path}\")\n        invalid_images += 1\n        continue\n        \n    #image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    #show the original image\n    plotted_img = image\n    plt.figure(figsize=(8, 6))\n    plt.imshow(plotted_img)\n    plt.axis('off')\n    plt.show()\n\n    results = model.predict(image, imgsz=640)\n\n    for result in results[0].boxes:\n        x1, y1, x2, y2 = map(int, result.xyxy[0])\n        class_id = int(result.cls[0])\n        label = classes[class_id]\n\n        #get the license plate\n        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n        #cv2.putText(image, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (152, 215, 235), 2)\n\n        #show the image with the license plate\n        plotted_img = image\n        plt.figure(figsize=(8, 6))\n        plt.imshow(plotted_img)\n        plt.axis('off')\n        plt.show()\n            \n       # Ensure bounding box is within image boundaries\n        h, w, _ = image.shape\n        x1, y1, x2, y2 = max(0, x1), max(0, y1), min(w, x2), min(h, y2)\n\n        # Crop the license plate\n        cropped_plate = image[y1:y2, x1:x2]\n        \n       # Display cropped license plate using Matplotlib\n        plt.figure(figsize=(4, 2))\n        plt.imshow(cv2.cvtColor(cropped_plate, cv2.COLOR_BGR2RGB))  # Convert BGR to RGB for Matplotlib\n        plt.axis(\"off\")\n        plt.title(\"Cropped License Plate\")\n        plt.show()\n   \n        # Initialize EasyOCR reader\n        \n        #reader = easyocr.Reader(['en'])\n\n        # Run OCR on cropped plate\n        #ocr_text = reader.readtext(cropped_plate, detail=0)\n\n        logging.getLogger(\"ppocr\").setLevel(logging.ERROR)  # Suppress PaddleOCR debug logs\n        ocr = PaddleOCR(use_angle_cls=True, lang=\"en\")\n\n        # Run OCR\n        ocr_result = ocr.ocr(cropped_plate, cls=True)\n\n        # Extract license plate text\n        license_plate_text = extract_license_plate(ocr_result)\n\n        #license_plate_text = ''.join(ocr_text).replace(\" \", \"\")\n\n        \n        print(\"Detected license_plate_text \",license_plate_text)\n        print(\"ground_truth\",ground_truth)\n\n        # Overlay detected text on the bounding box\n        cv2.putText(image, license_plate_text, (x1, y1 - 10), \n                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n        \n        # Show the final image with text overlay\n        plt.figure(figsize=(8, 6))\n        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n        plt.axis('off')\n        plt.title(\"Detected License Plate with OCR Text\")\n        plt.show()\n    \n        # Character-level accuracy\n        #char_accuracy = character_level_accuracy(ocr_result, ground_truth) * 100\n        #character_accuracy_sum += char_accuracy\n        #print(f\"Character-level accuracy: {char_accuracy:.2f}%\")\n        #print(f\"Running Character-level accuracy: {character_accuracy_sum:.2f}%\")\n        \n        # Word-level accuracy\n        word_accuracy = word_level_accuracy(license_plate_text, ground_truth) * 100\n        print(f\"Word-level accuracy: {word_accuracy}%\")\n        if word_accuracy == 100:\n            correct_predictions += 1\n\n        lev_accuracy = levenshtein_distance_accuracy(license_plate_text, ground_truth) * 100\n        levenshtein_accuracy_sum += lev_accuracy\n        \n        # Store result\n        #detected_plates.append(\" \".join(ocr_result))\n\n        # Print detected license plates\n        print(\"correct_predictions so far:\", correct_predictions)\n        print(\"levenshtein_accuracy_sum:\", levenshtein_accuracy_sum)\n\n# Calculate final accuracy scores\nvalid_images = len(df_google)-invalid_images\nword_level_accuracy = (correct_predictions / valid_images) * 100\naverage_levenshtein_accuracy = (levenshtein_accuracy_sum / valid_images)\n\n#character_level_accuracy = (character_accuracy_sum / len(test_df)) \n\nprint(f\"✅ Word-Level Accuracy: {word_level_accuracy:.2f}%\")\nprint(f\"✅ Levenshtein-Based Accuracy: {average_levenshtein_accuracy:.2f}%\")\nprint(f\"invalid images :\",invalid_images)\nprint(f\"valid images :\",valid_images)\n#print(f\"✅ Character-Level Accuracy: {character_level_accuracy:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T13:22:24.199636Z","iopub.execute_input":"2025-02-21T13:22:24.199861Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom ultralytics import YOLO\nimport cv2\nimport os\nimport easyocr\nfrom difflib import SequenceMatcher\nfrom tqdm import tqdm\nimport logging\nimport Levenshtein\n\n# Initialize EasyOCR reader\nreader = easyocr.Reader(['en'])\n\nclasses = [\"Licence Plate\"]\n\n# List to store OCR results\ndetected_plates = []\n\n# Accuracy counters\ncorrect_predictions = 0\ntotal_characters = 0\ncharacter_accuracy_sum = 0\nlevenshtein_accuracy_sum = 0\n\n# Function to calculate character-level accuracy\ndef character_level_accuracy(ocr_text, ground_truth):\n    ocr_text = ''.join(ocr_text).replace(\" \", \"\").lower()\n    print(\"Final OCR Text:\", ocr_text)\n    ground_truth = ground_truth.replace(\" \", \"\").lower()\n    \n    correct_characters = sum(1 for i in range(min(len(ocr_text), len(ground_truth))) if ocr_text[i] == ground_truth[i])\n    \n    return correct_characters / len(ground_truth) if len(ground_truth) > 0 else 0\n\n# Function to calculate word-level accuracy (Exact match)\ndef word_level_accuracy(ocr_text, ground_truth):\n    return 1 if ocr_text.strip().upper() == ground_truth.strip().upper() else 0\n\n# Function to calculate Levenshtein Distance-based accuracy\ndef levenshtein_distance_accuracy(ocr_text, ground_truth):\n    distance = Levenshtein.distance(ocr_text, ground_truth)\n    max_len = max(len(ocr_text), len(ground_truth))\n    return (1 - distance / max_len) if max_len > 0 else 0\n\n# Function to extract license plate text using EasyOCR\ndef extract_license_plate_easyocr(ocr_result):\n    \"\"\"\n    Custom function to extract license plate text from EasyOCR results.\n    Modify this function to filter noise and improve recognition accuracy.\n    \"\"\"\n    extracted_text = []\n    for detection in ocr_result:\n        text = detection[1]  # Assuming EasyOCR returns (bbox, text, confidence)\n        extracted_text.append(text)\n    \n    return ''.join(extracted_text).replace(\" \", \"\").upper()\n\nprint(\"Reset variables\")\ncorrect_characters = 0\ninvalid_images = 0\n\n# Process each image\nfor index, row in tqdm(df_google.iterrows(), total=len(df_google)):\n    image_path = row[\"image_path\"]\n    ground_truth = row[\"plate_number\"]\n    \n    image = cv2.imread(image_path)\n    if image is None:\n        print(f\"⚠️ Error: Unable to load image at {image_path}\")\n        invalid_images += 1\n        continue\n        \n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    # Show original image\n    plt.figure(figsize=(8, 6))\n    plt.imshow(image)\n    plt.axis('off')\n    plt.show()\n\n    results = model.predict(image, imgsz=640)\n\n    for result in results[0].boxes:\n        x1, y1, x2, y2 = map(int, result.xyxy[0])\n        class_id = int(result.cls[0])\n        label = classes[class_id]\n\n        # Draw bounding box around license plate\n        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n\n        # Show image with bounding box\n        plt.figure(figsize=(8, 6))\n        plt.imshow(image)\n        plt.axis('off')\n        plt.show()\n            \n        # Ensure bounding box is within image boundaries\n        h, w, _ = image.shape\n        x1, y1, x2, y2 = max(0, x1), max(0, y1), min(w, x2), min(h, y2)\n\n        # Crop the license plate\n        cropped_plate = image[y1:y2, x1:x2]\n        \n        # Display cropped license plate\n        plt.figure(figsize=(4, 2))\n        plt.imshow(cv2.cvtColor(cropped_plate, cv2.COLOR_BGR2RGB))\n        plt.axis(\"off\")\n        plt.title(\"Cropped License Plate\")\n        plt.show()\n   \n        # Run OCR on cropped plate using EasyOCR\n        ocr_result = reader.readtext(cropped_plate, detail=1)  # Get full OCR details\n\n        # Extract license plate text using the custom function\n        license_plate_text = extract_license_plate_easyocr(ocr_result)\n\n        print(\"Detected License Plate:\", license_plate_text)\n        print(\"Ground Truth:\", ground_truth)\n\n        # Overlay detected text on the bounding box\n        cv2.putText(image, license_plate_text, (x1, y1 - 10), \n                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n        \n        # Show final image with text overlay\n        plt.figure(figsize=(8, 6))\n        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n        plt.axis('off')\n        plt.title(\"Detected License Plate with OCR Text\")\n        plt.show()\n    \n        # Word-level accuracy\n        word_accuracy = word_level_accuracy(license_plate_text, ground_truth) * 100\n        print(f\"Word-Level Accuracy: {word_accuracy}%\")\n        if word_accuracy == 100:\n            correct_predictions += 1\n\n        lev_accuracy = levenshtein_distance_accuracy(license_plate_text, ground_truth) * 100\n        levenshtein_accuracy_sum += lev_accuracy\n\n        print(\"Correct Predictions so far:\", correct_predictions)\n        print(\"Levenshtein Accuracy Sum:\", levenshtein_accuracy_sum)\n\n# Calculate final accuracy scores\nvalid_images = len(df_google) - invalid_images\nword_level_accuracy = (correct_predictions / valid_images) * 100\naverage_levenshtein_accuracy = (levenshtein_accuracy_sum / valid_images)\n\nprint(f\"✅ Word-Level Accuracy: {word_level_accuracy:.2f}%\")\nprint(f\"✅ Levenshtein-Based Accuracy: {average_levenshtein_accuracy:.2f}%\")\nprint(f\"Invalid Images:\", invalid_images)\nprint(f\"Valid Images:\", valid_images)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}